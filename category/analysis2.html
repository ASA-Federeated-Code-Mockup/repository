<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="generator" content="Pelican" />
        <title>SLRB - analysis</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <link rel="icon" type="image/x-icon" href="theme/images/icons/slrb.svg">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
	<script>
    		$(function(){
        		$('a').each(function(){
            			if ($(this).prop('href') == window.location.href) {
                			$(this).addClass('active'); $(this).parents('li').addClass('active');
            			}
        		});
    		});
	</script>

	<link rel="stylesheet" href="https://files.stork-search.net/basic.css" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <div><img src="https://www.slrb.net/theme/images/icons/slrb.svg" height="30px"/>&nbsp;&nbsp;&nbsp;<h1>Speech and Language Resource Bank</h1></div>
                <div class="topnav">
                                <a href="/pages/home.html">home</a>
                                <a href="/pages/search.html">search</a>
                                <a href="/category/analysis.html">analysis</a>
                                <a href="/category/data.html">data</a>
                                <a href="/category/education.html">education</a>
                                <a href="/category/experimentation.html">experimentation</a>
		</div>
        </header>
	<br>

<p class="paginator">
        <a href="/category/analysis.html"><img src="theme/images/icons/arrow-left-stop.png" alt="|<"></a>
        <a href="/category/analysis.html"><img src="theme/images/icons/arrow-left.png" alt="<"></a>
    Page 2 / 2
</p>
    
    <br>
	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/CLEESE.html" rel="bookmark" title="Permalink to Combinatorial Expressive Speech Engine">Combinatorial Expressive Speech Engine</a></h1>
		<h4> C.L.E.E.S.E. (Combinatorial Expressive Speech Engine) is a tool designed to generate an infinite number of natural-sounding, expressive variations around an original speech recording. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/juan-jose-burred.html">Juan Jos√© Burred</a>,&nbsp;
                                        <a class="url fn" href="/author/emmanuel-ponsot.html">Emmanuel Ponsot</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-03-05 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://cream.ircam.fr/?p=521">http://cream.ircam.fr/?p=521</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/language.html">language</a>,&nbsp; 
                                        <a href="/tag/speech.html">speech</a>,&nbsp; 
                                        <a href="/tag/pitch.html">pitch</a>,&nbsp; 
                                        <a href="/tag/french.html">French</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a>,&nbsp; 
                                        <a href="/tag/japanese.html">Japanese</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/TAALES.html" rel="bookmark" title="Permalink to Tool for the Automatic Analysis of Lexical Sophistication">Tool for the Automatic Analysis of Lexical Sophistication</a></h1>
		<h4> TAALES is a tool that measures over 400 classic and new indices of lexical sophistication, and includes indices related to a wide range of sub-constructs. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/kristopher-kyle.html">Kristopher Kyle</a>,&nbsp;
                                        <a class="url fn" href="/author/scott-crossley.html">Scott Crossley</a>,&nbsp;
                                        <a class="url fn" href="/author/cynthia-berger.html">Cynthia Berger</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-02-08 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://www.linguisticanalysistools.org/taales.html">https://www.linguisticanalysistools.org/taales.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/language.html">language</a>,&nbsp; 
                                        <a href="/tag/lexicon.html">lexicon</a>,&nbsp; 
                                        <a href="/tag/communication.html">communication</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/TISK.html" rel="bookmark" title="Permalink to Time Invariant String Kernel Model">Time Invariant String Kernel Model</a></h1>
		<h4> TISK is the Time Invariant String Model of (human) spoken word recognition. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/heejo-you.html">Heejo You</a>,&nbsp;
                                        <a class="url fn" href="/author/thomas-hannagan.html">Thomas Hannagan</a>,&nbsp;
                                        <a class="url fn" href="/author/james-magnuson.html">James Magnuson</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2017-12-15 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://github.com/maglab-uconn/TISK1.0">https://github.com/maglab-uconn/TISK1.0</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/word-recognition.html">word-recognition</a>,&nbsp; 
                                        <a href="/tag/phonetics.html">phonetics</a>,&nbsp; 
                                        <a href="/tag/python.html">python</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/pitch-perception.html" rel="bookmark" title="Permalink to Inharmonic Pitch perception">Inharmonic Pitch perception</a></h1>
		<h4> Pitch-related music and speech tasks using conventional harmonic sounds and inharmonic sounds whose frequencies lack a common F0. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/malinda-j-mcpherson-josh-h-mcdermott.html">Malinda J. McPherson & Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2017-12-11 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/Diversity_In_Pitch_Perception.html">http://mcdermottlab.mit.edu/Diversity_In_Pitch_Perception.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/pitch.html">pitch</a>,&nbsp; 
                                        <a href="/tag/frequency.html">frequency</a>,&nbsp; 
                                        <a href="/tag/music.html">music</a>,&nbsp; 
                                        <a href="/tag/speech.html">speech</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/box-task.html" rel="bookmark" title="Permalink to The Box Task">The Box Task</a></h1>
		<h4> The Box Task is a paradigm for the computerized assessment of visuospatial working memory. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/roy-p-c-kessels-albert-postma.html">Roy P. C. Kessels & Albert Postma</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2017-09-15 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://roykessels.nl/tests-and-software/box-task">https://roykessels.nl/tests-and-software/box-task</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/working-memory.html">working-memory</a>,&nbsp; 
                                        <a href="/tag/psychology.html">psychology</a>,&nbsp; 
                                        <a href="/tag/experiment.html">experiment</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a>,&nbsp; 
                                        <a href="/tag/dutch.html">Dutch</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/reverberation.html" rel="bookmark" title="Permalink to Reverberation">Reverberation</a></h1>
		<h4> A large-scale statistical analysis of real-world acoustics, revealing strong regularities of reverberation in natural scenes. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/james-traer-josh-h-mcdermott.html">James Traer & Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2016-11-29 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/Reverb/ReverbSummary.html">http://mcdermottlab.mit.edu/Reverb/ReverbSummary.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/sound.html">sound</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/periphery.html">periphery</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/MouseTracker.html" rel="bookmark" title="Permalink to MouseTracker">MouseTracker</a></h1>
		<h4> MouseTracker is a freely available, user-friendly software package that allows researchers to record and analyze hand movements traveling toward potential responses on the screen (via the x, y coordinates of the computer mouse). </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/jon-freeman-nalini-ambady.html">Jon Freeman & Nalini Ambady</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2016-10-05 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://www.mousetracker.org/">http://www.mousetracker.org/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/software.html">software</a>,&nbsp; 
                                        <a href="/tag/cognition.html">cognition</a>,&nbsp; 
                                        <a href="/tag/psychology.html">psychology</a>,&nbsp; 
                                        <a href="/tag/experiment.html">experiment</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/TimeStudio.html" rel="bookmark" title="Permalink to The TimeStudio Project">The TimeStudio Project</a></h1>
		<h4> The TimeStudio Project is dedicated to the behavioral and brain sciences and is a completely free scientific workflow tool. TimeStudio allow researchers to quickly build a sequence of analysis steps by using a modular plugin structure. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/par-nystrom.html">P√§r Nystr√∂m</a>,&nbsp;
                                        <a class="url fn" href="/author/terje-falck-ytter.html">Terje Falck-Ytter</a>,&nbsp;
                                        <a class="url fn" href="/author/gustaf-gredeback.html">Gustaf Gredeb√§ck</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2016-10-05 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://timestudioproject.com/">http://timestudioproject.com/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/psychology.html">psychology</a>,&nbsp; 
                                        <a href="/tag/experiment.html">experiment</a>,&nbsp; 
                                        <a href="/tag/matlab.html">MATLAB</a>,&nbsp; 
                                        <a href="/tag/neuroscience.html">neuroscience</a>,&nbsp; 
                                        <a href="/tag/database.html">database</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/similarity-neighborhood.html" rel="bookmark" title="Permalink to Similarity Neighborhood Calculator">Similarity Neighborhood Calculator</a></h1>
		<h4> A web-based calculator to compute phonological similarity neighborhoods (including neighborhood density and neighborhood frequency). </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/michael-vitevitch.html">Michael Vitevitch</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2016-10-05 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://calculator.ku.edu/density/about">https://calculator.ku.edu/density/about</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/neighborhood-density.html">neighborhood-density</a>,&nbsp; 
                                        <a href="/tag/phonological-neighbors.html">phonological-neighbors</a>,&nbsp; 
                                        <a href="/tag/phonetics.html">phonetics</a>,&nbsp; 
                                        <a href="/tag/psychology.html">psychology</a>,&nbsp; 
                                        <a href="/tag/arabic.html">Arabic</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a>,&nbsp; 
                                        <a href="/tag/spanish.html">Spanish</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/natural-sounds.html" rel="bookmark" title="Permalink to Natural Sounds Stimulus Set">Natural Sounds Stimulus Set</a></h1>
		<h4> The sound set includes 165 natural sounds, each 2-seconds in duration. The sounds were intended to include many of the sounds people commonly hear in their daily life. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/sam-norman-haignere.html">Sam Norman-Haignere</a>,&nbsp;
                                        <a class="url fn" href="/author/nancy-g-kanwisher.html">Nancy G. Kanwisher</a>,&nbsp;
                                        <a class="url fn" href="/author/josh-h-mcdermott.html">Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2015-11-05 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/svnh/Natural-Sound/Stimuli.html">http://mcdermottlab.mit.edu/svnh/Natural-Sound/Stimuli.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/speech.html">speech</a>,&nbsp; 
                                        <a href="/tag/music.html">music</a>,&nbsp; 
                                        <a href="/tag/stimuli.html">stimuli</a>,&nbsp; 
                                        <a href="/tag/frequency.html">frequency</a>,&nbsp; 
                                        <a href="/tag/german.html">German</a>,&nbsp; 
                                        <a href="/tag/french.html">French</a>,&nbsp; 
                                        <a href="/tag/italian.html">Italian</a>,&nbsp; 
                                        <a href="/tag/russian.html">Russian</a>,&nbsp; 
                                        <a href="/tag/hindi.html">Hindi</a>,&nbsp; 
                                        <a href="/tag/chinese.html">Chinese</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/DAVID.html" rel="bookmark" title="Permalink to Da Amazing Voice Inflection Device">Da Amazing Voice Inflection Device</a></h1>
		<h4> DAVID (Da Amazing Voice Inflection Device) is a free, real-time voice transformation tool able to ‚Äúcolour‚Äù any voice recording with an emotion that wasn‚Äôt intended by its speaker. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/marco-liuni.html">Marco Liuni</a>,&nbsp;
                                        <a class="url fn" href="/author/petter-johansson.html">Petter Johansson</a>,&nbsp;
                                        <a class="url fn" href="/author/lars-hall.html">Lars Hall</a>,&nbsp;
                                        <a class="url fn" href="/author/rodrigo-segnini.html">Rodrigo Segnini</a>,&nbsp;
                                        <a class="url fn" href="/author/katsumi-watanabe.html">Katsumi Watanabe</a>,&nbsp;
                                        <a class="url fn" href="/author/daniel-richardson.html">Daniel Richardson</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2015-09-21 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://cream.ircam.fr/?p=44">http://cream.ircam.fr/?p=44</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/language.html">language</a>,&nbsp; 
                                        <a href="/tag/emotion.html">emotion</a>,&nbsp; 
                                        <a href="/tag/psychology.html">psychology</a>,&nbsp; 
                                        <a href="/tag/neuroscience.html">neuroscience</a>,&nbsp; 
                                        <a href="/tag/french.html">French</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a>,&nbsp; 
                                        <a href="/tag/swedish.html">Swedish</a>,&nbsp; 
                                        <a href="/tag/japanese.html">Japanese</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/attentive-tracking.html" rel="bookmark" title="Permalink to Attentive Tracking of Sound Sources">Attentive Tracking of Sound Sources</a></h1>
		<h4> Humans track sound sources through feature space with a movable focus of attention. Attentive tracking aids segregation of similar sound sources. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/kevin-jp-woods-josh-h-mcdermott.html">Kevin J.P. Woods & Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2015-08-31 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/attentive_tracking/index.html">http://mcdermottlab.mit.edu/attentive_tracking/index.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/sound-source.html">sound-source</a>,&nbsp; 
                                        <a href="/tag/speech.html">speech</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/mtap.html" rel="bookmark" title="Permalink to The Maryland Tongue Analysis Package">The Maryland Tongue Analysis Package</a></h1>
		<h4> The Maryland Tongue Analysis Package (MTAP), containing EdgeTrak and Surfaces, was developed by the Vocal Tract Visualization Laboratory for use with ultrasound images of the tongue. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/the-vocal-tract-vizualization-laboratory.html">The Vocal Tract Vizualization Laboratory</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2013-07-01 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://www.dental.umaryland.edu/speech/software/">https://www.dental.umaryland.edu/speech/software/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/tongue.html">tongue</a>,&nbsp; 
                                        <a href="/tag/palate.html">palate</a>,&nbsp; 
                                        <a href="/tag/ultrasound.html">ultrasound</a>,&nbsp; 
                                        <a href="/tag/diagrams.html">diagrams</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/texture.html" rel="bookmark" title="Permalink to Auditory Textures">Auditory Textures</a></h1>
		<h4> Texture properties could be captured by summary statistics that are time-averages of acoustic measurements. We have explored the hypothesis that the auditory system represents textures in this way, with statistics of the measurements made in the peripheral auditory system. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/josh-h-mcdermott-eero-p-simoncelli.html">Josh H. McDermott & Eero P. Simoncelli</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2013-02-24 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/texture_examples/index.html">http://mcdermottlab.mit.edu/texture_examples/index.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/sound.html">sound</a>,&nbsp; 
                                        <a href="/tag/perception.html">perception</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/source-repetition.html" rel="bookmark" title="Permalink to Source Repetition Stimuli">Source Repetition Stimuli</a></h1>
		<h4> We used a simple generative model to synthesize novel sounds with naturalistic properties. We found that such sounds could be segregated and identified if they occurred more than once across different mixtures, even when the same sounds were impossible to segregate in single mixtures. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/josh-h-mcdermott.html">Josh H. McDermott</a>,&nbsp;
                                        <a class="url fn" href="/author/david-wrobleski.html">David Wrobleski</a>,&nbsp;
                                        <a class="url fn" href="/author/andrew-j-oxenham.html">Andrew J. Oxenham</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2011-01-18 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/source_repetition/index.html">http://mcdermottlab.mit.edu/source_repetition/index.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/speech.html">speech</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/sound-sources.html">sound-sources</a>,&nbsp; 
                                        <a href="/tag/acoustics.html">acoustics</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/music-scrambling.html" rel="bookmark" title="Permalink to Music Scrambling">Music Scrambling</a></h1>
		<h4> Example stimuli of music scrambled by pitch, rhythm, and both. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/sam-norman-haignere.html">Sam Norman-Haignere</a>,&nbsp;
                                        <a class="url fn" href="/author/nancy-kanwisher.html">Nancy Kanwisher</a>,&nbsp;
                                        <a class="url fn" href="/author/josh-h-mcdermott.html">Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2010-12-31 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/music_scrambling/index.html">http://mcdermottlab.mit.edu/music_scrambling/index.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/pitch.html">pitch</a>,&nbsp; 
                                        <a href="/tag/harmonics.html">harmonics</a>,&nbsp; 
                                        <a href="/tag/frequency.html">frequency</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/consonance.html" rel="bookmark" title="Permalink to Consonance">Consonance</a></h1>
		<h4> A study into the underlying mechanisms of consonance. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/josh-h-mcdermott.html">Josh H. McDermott</a>,&nbsp;
                                        <a class="url fn" href="/author/andriana-j-lehr.html">Andriana J. Lehr</a>,&nbsp;
                                        <a class="url fn" href="/author/andrew-j-oxenham.html">Andrew J. Oxenham</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2010-06-08 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/consonance_examples/index.html">http://mcdermottlab.mit.edu/consonance_examples/index.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/frequency.html">frequency</a>,&nbsp; 
                                        <a href="/tag/harmony.html">harmony</a>,&nbsp; 
                                        <a href="/tag/acoustics.html">acoustics</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/cocktail-party.html" rel="bookmark" title="Permalink to Cocktail Party Problem">Cocktail Party Problem</a></h1>
		<h4> The cocktail party problem is the task of hearing a sound of interest, often a speech signal, in a complex auditory setting. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/josh-h-mcdermott.html">Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2009-10-05 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/cocktail_examples/index.html">http://mcdermottlab.mit.edu/cocktail_examples/index.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/speech.html">speech</a>,&nbsp; 
                                        <a href="/tag/noise.html">noise</a>,&nbsp; 
                                        <a href="/tag/frequency.html">frequency</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/sound-contour.html" rel="bookmark" title="Permalink to Sound Contour Demos">Sound Contour Demos</a></h1>
		<h4> A study of relative pitch - the relationships between the pitch of successive sounds - to determine whether properties of relative pitch would generalize to other auditory dimensions. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/josh-h-mcdermott.html">Josh H. McDermott</a>,&nbsp;
                                        <a class="url fn" href="/author/andriana-j.html">Andriana J.</a>,&nbsp;
                                        <a class="url fn" href="/author/andrew-j-oxenham.html">Andrew J. Oxenham</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2008-05-09 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/sound_contour_demos/sound_contour_demos.html">http://mcdermottlab.mit.edu/sound_contour_demos/sound_contour_demos.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/frequency.html">frequency</a>,&nbsp; 
                                        <a href="/tag/pitch.html">pitch</a>,&nbsp; 
                                        <a href="/tag/harmony.html">harmony</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/spectral-completion.html" rel="bookmark" title="Permalink to Spectral Completion of Paritally Masked Sounds">Spectral Completion of Paritally Masked Sounds</a></h1>
		<h4> The auditory system uses the audible portions of an object's spectrum to infer the portions that are likely to have been masked, and that are thus not veridically present in the input to the auditory system. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/josh-h-mcdermott-andrew-j-oxenham.html">Josh H. McDermott & Andrew J. Oxenham</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2008-02-15 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/spec_comp_demos/spec_comp_page1.html">http://mcdermottlab.mit.edu/spec_comp_demos/spec_comp_page1.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/sound-sources.html">sound-sources</a>,&nbsp; 
                                        <a href="/tag/masking.html">masking</a>,&nbsp; 
                                        <a href="/tag/perception.html">perception</a> 
        </div>

        <br>


<p class="paginator">
        <a href="/category/analysis.html"><img src="theme/images/icons/arrow-left-stop.png" alt="|<"></a>
        <a href="/category/analysis.html"><img src="theme/images/icons/arrow-left.png" alt="<"></a>
    Page 2 / 2
</p>

  
        <footer id="contentinfo" class="body">
                <!-- <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address>

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p> -->
        </footer><!-- /#contentinfo -->

	<script src="https://files.stork-search.net/releases/v1.5.0/stork.js"></script>
	<script>
		stork.register("sitesearch", "/search-index.st")
	</script>
</body>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-8XWFESHEMV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());  gtag('config', 'G-8XWFESHEMV');
</script>
  
</html>