<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="generator" content="Pelican" />
        <title>SLRB - analysis</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <link rel="icon" href="https://slrb.net/theme/images/icons/slrb.svg" type="image/x-icon">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
	<script>
    		$(function(){
        		$('a').each(function(){
            			if ($(this).prop('href') == window.location.href) {
                			$(this).addClass('active'); $(this).parents('li').addClass('active');
            			}
        		});
    		});
	</script>

</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <div><img src="https://www.slrb.net/theme/images/icons/slrb.svg" height="30px"/>&nbsp;&nbsp;&nbsp;<h1>Speech and Language Resource Bank</h1></div>
                <div class="topnav">
                                <a href="/pages/home.html">home</a>
                                <a href="/category/analysis.html">analysis</a>
                                <a href="/category/data.html">data</a>
                                <a href="/category/education.html">education</a>
                                <a href="/category/experimentation.html">experimentation</a>
		</div>
        </header>
	<br>

<p class="paginator">
    Page 1 / 2
        <a href="/category/analysis2.html"><i class="fa-solid fa-arrow-circle-right"></i></a>
        <a href="/category/analysis2.html"><i class="fa-solid fa-arrow-right-to-line"></i></a>
</p>
    
    <br>
    
	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/FreeSurfer.html" rel="bookmark" title="Permalink to FreeSurfer">FreeSurfer</a></h1>
		<h4> FreeSurfer is an open source package for the analysis and visualization of structural, functional, and diffusion neuroimaging data from cross-sectional and longitudinal studies. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/bruce-fischl.html">Bruce Fischl</a>,&nbsp;
                                        <a class="url fn" href="/author/anders-dale.html">Anders Dale</a>,&nbsp;
                                        <a class="url fn" href="/author/martin-sereno.html">Martin Sereno</a>,&nbsp;
                                        <a class="url fn" href="/author/doug-greve.html">Doug Greve</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2022-01-13 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://freesurfer.net/">https://freesurfer.net/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/neuroimaging.html">neuroimaging</a>,&nbsp; 
                                        <a href="/tag/psychology.html">psychology</a>,&nbsp; 
                                        <a href="/tag/neuropsychology.html">neuropsychology</a>,&nbsp; 
                                        <a href="/tag/toolbox.html">toolbox</a>,&nbsp; 
                                        <a href="/tag/data.html">data</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/case-studies-python.html" rel="bookmark" title="Permalink to Case-Studies-Python">Case-Studies-Python</a></h1>
		<h4> An introduction to the practicing neuroscientist to data analysis in Python. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/emily-schlafly.html">Emily Schlafly</a>,&nbsp;
                                        <a class="url fn" href="/author/mark-kramer.html">Mark Kramer</a>,&nbsp;
                                        <a class="url fn" href="/author/anthea-cheung.html">Anthea Cheung</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2022-01-13 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://github.com/Mark-Kramer/Case-Studies-Python/">https://github.com/Mark-Kramer/Case-Studies-Python/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/python.html">Python</a>,&nbsp; 
                                        <a href="/tag/matlab.html">matlab</a>,&nbsp; 
                                        <a href="/tag/neuroscience.html">neuroscience</a>,&nbsp; 
                                        <a href="/tag/experiment.html">experiment</a>,&nbsp; 
                                        <a href="/tag/eeg.html">EEG</a>,&nbsp; 
                                        <a href="/tag/data.html">data</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/MNE-Python.html" rel="bookmark" title="Permalink to MNE-Python">MNE-Python</a></h1>
		<h4> Open-source Python package for exploring, visualizing, and analyzing human neurophysiological data: MEG, EEG, sEEG, ECoG, NIRS, and more. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/alexandre-gramfort.html">Alexandre Gramfort</a>,&nbsp;
                                        <a class="url fn" href="/author/martin-luessi.html">Martin Luessi</a>,&nbsp;
                                        <a class="url fn" href="/author/eric-larson.html">Eric Larson</a>,&nbsp;
                                        <a class="url fn" href="/author/denis-a-engemann.html">Denis A. Engemann</a>,&nbsp;
                                        <a class="url fn" href="/author/daniel-strohmeier.html">Daniel Strohmeier</a>,&nbsp;
                                        <a class="url fn" href="/author/christian-brodbeck.html">Christian Brodbeck</a>,&nbsp;
                                        <a class="url fn" href="/author/roman-goj.html">Roman Goj</a>,&nbsp;
                                        <a class="url fn" href="/author/mainak-jas.html">Mainak Jas</a>,&nbsp;
                                        <a class="url fn" href="/author/teon-brooks.html">Teon Brooks</a>,&nbsp;
                                        <a class="url fn" href="/author/lauri-parkkonen.html">Lauri Parkkonen</a>,&nbsp;
                                        <a class="url fn" href="/author/matti-s-hamalainen.html">Matti S. Hämäläinen</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2021-12-02 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://mne.tools/stable/index.html">https://mne.tools/stable/index.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/experiment.html">experiment</a>,&nbsp; 
                                        <a href="/tag/python.html">Python</a>,&nbsp; 
                                        <a href="/tag/data.html">data</a>,&nbsp; 
                                        <a href="/tag/machine-learning.html">machine-learning</a>,&nbsp; 
                                        <a href="/tag/neuroscience.html">neuroscience</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/soundgen.html" rel="bookmark" title="Permalink to Soundgen">Soundgen</a></h1>
		<h4> Soundgen is an open-source toolbox for voice synthesis, manipulation, and analysis. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/andrey-anikin.html">Andrey Anikin</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2021-11-21 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://CRAN.R-project.org/package=soundgen">https://CRAN.R-project.org/package=soundgen</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/sound-synthesis.html">sound-synthesis</a>,&nbsp; 
                                        <a href="/tag/pitch.html">pitch</a>,&nbsp; 
                                        <a href="/tag/experiment.html">experiment</a>,&nbsp; 
                                        <a href="/tag/frequency.html">frequency</a>,&nbsp; 
                                        <a href="/tag/programming.html">programming</a>,&nbsp; 
                                        <a href="/tag/acoustics.html">acoustics</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/ALICE.html" rel="bookmark" title="Permalink to Automatic Linguistic Unit Count Estimator (ALICE)">Automatic Linguistic Unit Count Estimator (ALICE)</a></h1>
		<h4> ALICE is a tool for estimating the number of adult-spoken linguistic units from child-centered audio recordings, as captured by microphones worn by children. It is meant as an open-source alternative for LENA adult word count (AWC) estimator. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/okko-rasanen.html">Okko Räsänen</a>,&nbsp;
                                        <a class="url fn" href="/author/shreyas-seshadri.html">Shreyas Seshadri</a>,&nbsp;
                                        <a class="url fn" href="/author/marvin-lavechin.html">Marvin Lavechin</a>,&nbsp;
                                        <a class="url fn" href="/author/alejandrina-cristia.html">Alejandrina Cristia</a>,&nbsp;
                                        <a class="url fn" href="/author/marisa-casillas.html">Marisa Casillas</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2021-11-02 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://github.com/orasanen/ALICE">https://github.com/orasanen/ALICE</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/language.html">language</a>,&nbsp; 
                                        <a href="/tag/linguistics.html">linguistics</a>,&nbsp; 
                                        <a href="/tag/phonetics.html">phonetics</a>,&nbsp; 
                                        <a href="/tag/speech-production.html">speech-production</a>,&nbsp; 
                                        <a href="/tag/argentinian-spanish.html">Argentinian Spanish</a>,&nbsp; 
                                        <a href="/tag/tseltal.html">Tseltal</a>,&nbsp; 
                                        <a href="/tag/yeli-dnye.html">Yélî Dnye</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/MPTinR.html" rel="bookmark" title="Permalink to MPTinR">MPTinR</a></h1>
		<h4> Provides a user-friendly way for the analysis of multinomial processing tree (MPT) models. Multinomial modeling and the measurement of cognitive processes for single and multiple datasets. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/henrik-singmann.html">Henrik Singmann</a>,&nbsp;
                                        <a class="url fn" href="/author/david-kellen.html">David Kellen</a>,&nbsp;
                                        <a class="url fn" href="/author/quentin-gronau.html">Quentin Gronau</a>,&nbsp;
                                        <a class="url fn" href="/author/christian-mueller.html">Christian Mueller</a>,&nbsp;
                                        <a class="url fn" href="/author/akhil-s-bhel.html">Akhil S. Bhel</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2021-07-13 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://CRAN.R-project.org/package=MPTinR">https://CRAN.R-project.org/package=MPTinR</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/cognition.html">cognition</a>,&nbsp; 
                                        <a href="/tag/experiment.html">experiment</a>,&nbsp; 
                                        <a href="/tag/software.html">software</a>,&nbsp; 
                                        <a href="/tag/r.html">R</a>,&nbsp; 
                                        <a href="/tag/psychology.html">psychology</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/pair-test.html" rel="bookmark" title="Permalink to The Pair Test">The Pair Test</a></h1>
		<h4> The Pair Test offers improved diagnosis, with more precise indication on the nature of the memory deficit and the underlying processing impairment in patients with known or suspected pathology in the temporal lobe. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/sarah-buck.html">Sarah Buck</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2021-05-07 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://www.protocols.io/view/the-pair-test-a-computerised-measure-of-learning-a-bjvckn2w">https://www.protocols.io/view/the-pair-test-a-computerised-measure-of-learning-a-bjvckn2w</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/memory.html">memory</a>,&nbsp; 
                                        <a href="/tag/psychology.html">psychology</a>,&nbsp; 
                                        <a href="/tag/learning.html">learning</a>,&nbsp; 
                                        <a href="/tag/neuropsychology.html">neuropsychology</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/ANLffr.html" rel="bookmark" title="Permalink to ANL Frequency-Following Responses">ANL Frequency-Following Responses</a></h1>
		<h4> A set of tools to analyze and interpret auditory steady-state responses, particularly the subcortical kind commonly known as frequency-following responses (FFRs). </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/hari-bharadwaj.html">Hari Bharadwaj</a>,&nbsp;
                                        <a class="url fn" href="/author/hao-lu.html">Hao Lu</a>,&nbsp;
                                        <a class="url fn" href="/author/lenny-varghese.html">Lenny Varghese</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2021-04-04 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://github.com/SNAPsoftware/ANLffr">https://github.com/SNAPsoftware/ANLffr</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/frequency.html">frequency</a>,&nbsp; 
                                        <a href="/tag/python.html">Python</a>,&nbsp; 
                                        <a href="/tag/phase-locking.html">phase-locking</a>,&nbsp; 
                                        <a href="/tag/eeg.html">EEG</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> tested&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/phonetics-jl.html" rel="bookmark" title="Permalink to Phonetics.jl">Phonetics.jl</a></h1>
		<h4> A collection of functions to analyze phonetic data in Julia. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/matthew-c-kelley.html">Matthew C. Kelley</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2021-02-08 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://github.com/maetshju/phonetics.jl">https://github.com/maetshju/phonetics.jl</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/julia.html">julia</a>,&nbsp; 
                                        <a href="/tag/vowels.html">vowels</a>,&nbsp; 
                                        <a href="/tag/plotting.html">plotting</a>,&nbsp; 
                                        <a href="/tag/lexicon.html">lexicon</a>,&nbsp; 
                                        <a href="/tag/acoustic-distance.html">acoustic-distance</a>,&nbsp; 
                                        <a href="/tag/neighborhood-density.html">neighborhood-density</a>,&nbsp; 
                                        <a href="/tag/phonotactic-probability.html">phonotactic-probability</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/NAF.html" rel="bookmark" title="Permalink to Nasalization from Acoustic Features (NAF)">Nasalization from Acoustic Features (NAF)</a></h1>
		<h4> R code implementing a methodology for the automatic measurement of vowel nasalization from acoustic data. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/christopher-carignan.html">Christopher Carignan</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2021-02-04 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://github.com/ChristopherCarignan/NAF/">https://github.com/ChristopherCarignan/NAF/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/nasalization.html">nasalization</a>,&nbsp; 
                                        <a href="/tag/phonetics.html">phonetics</a>,&nbsp; 
                                        <a href="/tag/machine-learning.html">machine-learning</a>,&nbsp; 
                                        <a href="/tag/mfcc.html">MFCC</a>,&nbsp; 
                                        <a href="/tag/acoustics.html">acoustics</a>,&nbsp; 
                                        <a href="/tag/r.html">R</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/SemDis.html" rel="bookmark" title="Permalink to SemDis">SemDis</a></h1>
		<h4> SemDis uses advances in natural language processing to automatically determine how closely associated texts are to each other. Higher SemDis scores indicate two texts are less related, that is, they are more distantly related ideas or concepts. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/dan-johnson-roger-beaty.html">Dan Johnson & Roger Beaty</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-09-29 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://semdis.wlu.psu.edu/">http://semdis.wlu.psu.edu/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/language.html">language</a>,&nbsp; 
                                        <a href="/tag/semantics.html">semantics</a>,&nbsp; 
                                        <a href="/tag/word-recognition.html">word-recognition</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/auditory-grouping-cues.html" rel="bookmark" title="Permalink to Auditory Grouping Cues">Auditory Grouping Cues</a></h1>
		<h4> Here, we derive auditory grouping cues by measuring and summarizing statistics of natural sound features. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/wiktor-mlynarski.html">Wiktor Młynarski</a>,&nbsp;
                                        <a class="url fn" href="/author/josh-h-mcdermott.html">Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2019-12-10 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/grouping_statistics/index.html">http://mcdermottlab.mit.edu/grouping_statistics/index.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/sensory.html">sensory</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/frequency.html">frequency</a>,&nbsp; 
                                        <a href="/tag/harmony.html">harmony</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/illusory-texture.html" rel="bookmark" title="Permalink to Illusory Texture Demos">Illusory Texture Demos</a></h1>
		<h4> Here you will find some sound examples demonstrating the phenomenon of "illusory sound texture." </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/richard-mcwalter-and-josh-mcdermott.html">Richard McWalter and Josh McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2019-11-29 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/textcont.html">http://mcdermottlab.mit.edu/textcont.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/sound-texture.html">sound-texture</a>,&nbsp; 
                                        <a href="/tag/perception.html">perception</a>,&nbsp; 
                                        <a href="/tag/speech.html">speech</a>,&nbsp; 
                                        <a href="/tag/music.html">music</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/CRIE.html" rel="bookmark" title="Permalink to Chinese Readability Index Explorer (CRIE)">Chinese Readability Index Explorer (CRIE)</a></h1>
		<h4> The Chinese Readability Index Explorer (CRIE) is composed of four subsystems and incorporates 82 multilevel linguistic features. CRIE is able to conduct the major tasks of segmentation, syntactic parsing, and feature extraction. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/yao-ting-sung.html">Yao-Ting Sung</a>,&nbsp;
                                        <a class="url fn" href="/author/tao-hsing-chang.html">Tao-Hsing Chang</a>,&nbsp;
                                        <a class="url fn" href="/author/wei-chun-lin.html">Wei-Chun Lin</a>,&nbsp;
                                        <a class="url fn" href="/author/kuan-sheng-hsieh.html">Kuan-Sheng Hsieh</a>,&nbsp;
                                        <a class="url fn" href="/author/kuo-en-chang.html">Kuo-En Chang</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2019-09-29 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://www.chinesereadability.net/CRIE/?LANG=CHT">http://www.chinesereadability.net/CRIE/?LANG=CHT</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/linguistics.html">linguistics</a>,&nbsp; 
                                        <a href="/tag/syntax.html">syntax</a>,&nbsp; 
                                        <a href="/tag/phonetics.html">phonetics</a>,&nbsp; 
                                        <a href="/tag/machine-learning.html">machine-learning</a>,&nbsp; 
                                        <a href="/tag/chinese.html">Chinese</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/model-match.html" rel="bookmark" title="Permalink to Model-Matched Sounds">Model-Matched Sounds</a></h1>
		<h4> Cochleograms and sound files are shown for example stimuli from the model-matching experiment. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/sam-v-norman-haignere-josh-h-mcdermott.html">Sam V. Norman-Haignere & Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-12-03 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/svnh/model-matching/Stimuli_from_Model-Matching_Experiment.html">http://mcdermottlab.mit.edu/svnh/model-matching/Stimuli_from_Model-Matching_Experiment.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/sensory.html">sensory</a>,&nbsp; 
                                        <a href="/tag/auditory-cortex.html">auditory-cortex</a>,&nbsp; 
                                        <a href="/tag/neuroscience.html">neuroscience</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/phonotactic-probability-calculator.html" rel="bookmark" title="Permalink to Phonotactic Probability Calculator">Phonotactic Probability Calculator</a></h1>
		<h4> Used to calculate the phonotactic probabilities of real English, Arabic, and Spanish words or specially constructed nonwords. Analyses can be performed on words or nonwords up to 17 phonemes in length. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/michael-vitevitch.html">Michael Vitevitch</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-09-29 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://calculator.ku.edu/phonotactic/about">https://calculator.ku.edu/phonotactic/about</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/phonotactic-probability.html">phonotactic-probability</a>,&nbsp; 
                                        <a href="/tag/psychology.html">psychology</a>,&nbsp; 
                                        <a href="/tag/phonetics.html">phonetics</a>,&nbsp; 
                                        <a href="/tag/language-processing.html">language-processing</a>,&nbsp; 
                                        <a href="/tag/arabic.html">Arabic</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a>,&nbsp; 
                                        <a href="/tag/spanish.html">Spanish</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/inharmonic-speech-segregation.html" rel="bookmark" title="Permalink to Inharmonic Speech Segregation">Inharmonic Speech Segregation</a></h1>
		<h4> Inharmonic speech demos showing sound segregation. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/sara-popham.html">Sara Popham</a>,&nbsp;
                                        <a class="url fn" href="/author/dana-boebinger.html">Dana Boebinger</a>,&nbsp;
                                        <a class="url fn" href="/author/dan-p-w-ellis.html">Dan P. W. Ellis</a>,&nbsp;
                                        <a class="url fn" href="/author/hideki-kawahara.html">Hideki Kawahara</a>,&nbsp;
                                        <a class="url fn" href="/author/josh-h-mcdermott.html">Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-05-29 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/inharmonic_speech_examples/index.html">http://mcdermottlab.mit.edu/inharmonic_speech_examples/index.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/sound-sources.html">sound-sources</a>,&nbsp; 
                                        <a href="/tag/brain.html">brain</a>,&nbsp; 
                                        <a href="/tag/frequency.html">frequency</a>,&nbsp; 
                                        <a href="/tag/harmonics.html">harmonics</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/texture-time-averaging.html" rel="bookmark" title="Permalink to Texture-Time Averaging">Texture-Time Averaging</a></h1>
		<h4> Audio files showing the adaptive and selective time-averaging of auditory scenes. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/richard-mcwalter-josh-mcdermott.html">Richard McWalter & Josh McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-05-07 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/textint.html">http://mcdermottlab.mit.edu/textint.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/perception.html">perception</a>,&nbsp; 
                                        <a href="/tag/sensory-input.html">sensory-input</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/schema-learning.html" rel="bookmark" title="Permalink to Schema Learning for the Cocktail Party Problem">Schema Learning for the Cocktail Party Problem</a></h1>
		<h4> The cocktail party problem requires listeners to infer individual sound </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/kevin-jp-woods-josh-h-mcdermott.html">Kevin J.P. Woods & Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-04-03 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/schema_learning/index.html">http://mcdermottlab.mit.edu/schema_learning/index.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/sound-sources.html">sound-sources</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/schema.html">schema</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/CLEESE.html" rel="bookmark" title="Permalink to Combinatorial Expressive Speech Engine">Combinatorial Expressive Speech Engine</a></h1>
		<h4> C.L.E.E.S.E. (Combinatorial Expressive Speech Engine) is a tool designed to generate an infinite number of natural-sounding, expressive variations around an original speech recording. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/juan-jose-burred.html">Juan José Burred</a>,&nbsp;
                                        <a class="url fn" href="/author/emmanuel-ponsot.html">Emmanuel Ponsot</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-03-29 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://cream.ircam.fr/?p=521">http://cream.ircam.fr/?p=521</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/language.html">language</a>,&nbsp; 
                                        <a href="/tag/speech.html">speech</a>,&nbsp; 
                                        <a href="/tag/pitch.html">pitch</a>,&nbsp; 
                                        <a href="/tag/french.html">French</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a>,&nbsp; 
                                        <a href="/tag/japanese.html">Japanese</a> 
        </div>

        <br>


<p class="paginator">
    Page 1 / 2
        <a href="/category/analysis2.html"><i class="fa-solid fa-arrow-circle-right"></i></a>
        <a href="/category/analysis2.html"><i class="fa-solid fa-arrow-right-to-line"></i></a>
</p>

  
        <footer id="contentinfo" class="body">
                <!-- <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address>

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p> -->
        </footer><!-- /#contentinfo -->

</body>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-8XWFESHEMV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());  gtag('config', 'G-8XWFESHEMV');
</script>
  
</html>