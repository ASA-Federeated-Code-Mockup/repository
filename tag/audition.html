<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="generator" content="Pelican" />
        <title>SLRB - audition</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <link rel="icon" type="image/x-icon" href="theme/images/icons/slrb.svg">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
	<script>
    		$(function(){
        		$('a').each(function(){
            			if ($(this).prop('href') == window.location.href) {
                			$(this).addClass('active'); $(this).parents('li').addClass('active');
            			}
        		});
    		});
	</script>

	<link rel="stylesheet" href="https://files.stork-search.net/basic.css" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <div><img src="https://www.slrb.net/theme/images/icons/slrb.svg" height="30px"/>&nbsp;&nbsp;&nbsp;<h1>Speech and Language Resource Bank</h1></div>
                <div class="topnav">
                                <a href="/pages/home.html">home</a>
                                <a href="/pages/search.html">search</a>
                                <a href="/category/analysis.html">analysis</a>
                                <a href="/category/data.html">data</a>
                                <a href="/category/education.html">education</a>
                                <a href="/category/experimentation.html">experimentation</a>
		</div>
        </header>
	<br>

<!--  -->
<p class="paginator">
    <!--  -->
    Page 1 / 1
    <!--  -->
</p>
<!--  -->    
    <br>
	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/headphone-screen.html" rel="bookmark" title="Permalink to Efficient Headphone Screen">Efficient Headphone Screen</a></h1>
		<h4> Conducting online auditory experiments? A new headphone test to help you to efficiently screen out participants who probably aren't using headphones. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/alice-e-milne.html">Alice E. Milne</a>,&nbsp;
                                        <a class="url fn" href="/author/roberta-bianco.html">Roberta Bianco</a>,&nbsp;
                                        <a class="url fn" href="/author/katarina-c-poole.html">Katarina C. Poole</a>,&nbsp;
                                        <a class="url fn" href="/author/sijia-zhao.html">Sijia Zhao</a>,&nbsp;
                                        <a class="url fn" href="/author/andrew-j-oxenham.html">Andrew J. Oxenham</a>,&nbsp;
                                        <a class="url fn" href="/author/alexander-j-billig.html">Alexander J. Billig</a>,&nbsp;
                                        <a class="url fn" href="/author/maria-chait.html">Maria Chait</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2021-07-28 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://app.gorilla.sc/openmaterials/100917">https://app.gorilla.sc/openmaterials/100917</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/experiment.html">experiment</a>,&nbsp; 
                                        <a href="/tag/stimuli.html">stimuli</a>,&nbsp; 
                                        <a href="/tag/noise.html">noise</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/ANLffr.html" rel="bookmark" title="Permalink to ANL Frequency-Following Responses">ANL Frequency-Following Responses</a></h1>
		<h4> A set of tools to analyze and interpret auditory steady-state responses, particularly the subcortical kind commonly known as frequency-following responses (FFRs). </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/hari-bharadwaj.html">Hari Bharadwaj</a>,&nbsp;
                                        <a class="url fn" href="/author/hao-lu.html">Hao Lu</a>,&nbsp;
                                        <a class="url fn" href="/author/lenny-varghese.html">Lenny Varghese</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2021-04-04 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://github.com/SNAPsoftware/ANLffr">https://github.com/SNAPsoftware/ANLffr</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/frequency.html">frequency</a>,&nbsp; 
                                        <a href="/tag/python.html">Python</a>,&nbsp; 
                                        <a href="/tag/phase-locking.html">phase-locking</a>,&nbsp; 
                                        <a href="/tag/eeg.html">EEG</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/StimuliApp.html" rel="bookmark" title="Permalink to StimuliApp">StimuliApp</a></h1>
		<h4> StimuliApp is a free app designed to create psychophysical tests with precise timing on iOS and iPadOS devices. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/rafael-marin-campos.html">Rafael Marin-Campos</a>,&nbsp;
                                        <a class="url fn" href="/author/joseph-dalmau.html">Joseph Dalmau</a>,&nbsp;
                                        <a class="url fn" href="/author/albert-compte.html">Albert Compte</a>,&nbsp;
                                        <a class="url fn" href="/author/daniel-linares.html">Daniel Linares</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-10-05 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://www.stimuliapp.com/">https://www.stimuliapp.com/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/psychophysics.html">psychophysics</a>,&nbsp; 
                                        <a href="/tag/experiment.html">experiment</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/visual-stimulation.html">visual-stimulation</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/headphone-check.html" rel="bookmark" title="Permalink to Headphone Check">Headphone Check</a></h1>
		<h4> This code implements a headphone screening task intended to facilitate web-based experiments employing auditory stimuli. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/kevin-jp-woods.html">Kevin J.P. Woods</a>,&nbsp;
                                        <a class="url fn" href="/author/max-h-siegel.html">Max H. Siegel</a>,&nbsp;
                                        <a class="url fn" href="/author/james-traer.html">James Traer</a>,&nbsp;
                                        <a class="url fn" href="/author/josh-h-mcdermott.html">Josh H. McDermott</a>,&nbsp;
                                        <a class="url fn" href="/author/ray-gonzalez.html">Ray Gonzalez</a>,&nbsp;
                                        <a class="url fn" href="/author/kelsey-allen.html">Kelsey Allen</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-06-02 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://github.com/mcdermottLab/HeadphoneCheck">https://github.com/mcdermottLab/HeadphoneCheck</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/experiment.html">experiment</a>,&nbsp; 
                                        <a href="/tag/stimuli.html">stimuli</a>,&nbsp; 
                                        <a href="/tag/pure-tone.html">pure-tone</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/auditory-grouping-cues.html" rel="bookmark" title="Permalink to Auditory Grouping Cues">Auditory Grouping Cues</a></h1>
		<h4> Here, we derive auditory grouping cues by measuring and summarizing statistics of natural sound features. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/wiktor-mlynarski.html">Wiktor MÅ‚ynarski</a>,&nbsp;
                                        <a class="url fn" href="/author/josh-h-mcdermott.html">Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2019-12-10 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/grouping_statistics/index.html">http://mcdermottlab.mit.edu/grouping_statistics/index.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/sensory.html">sensory</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/frequency.html">frequency</a>,&nbsp; 
                                        <a href="/tag/harmony.html">harmony</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/AELP.html" rel="bookmark" title="Permalink to Auditory English Lexicon Project">Auditory English Lexicon Project</a></h1>
		<h4> The Auditory English Lexicon Project (AELP) is a multi-talker, multi-region psycholinguistic database of 10,170 spoken words and 10,170 spoken nonwords. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/winston-d-goh.html">Winston D. Goh</a>,&nbsp;
                                        <a class="url fn" href="/author/melvin-j-yap.html">Melvin J. Yap</a>,&nbsp;
                                        <a class="url fn" href="/author/qian-wen-chee.html">Qian Wen Chee</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2019-10-05 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://inetapps.nus.edu.sg/aelp/">https://inetapps.nus.edu.sg/aelp/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/psycholinguistics.html">psycholinguistics</a>,&nbsp; 
                                        <a href="/tag/database.html">database</a>,&nbsp; 
                                        <a href="/tag/lexicon.html">lexicon</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/semantics.html">semantics</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/model-match.html" rel="bookmark" title="Permalink to Model-Matched Sounds">Model-Matched Sounds</a></h1>
		<h4> Cochleograms and sound files are shown for example stimuli from the model-matching experiment. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/sam-v-norman-haignere-josh-h-mcdermott.html">Sam V. Norman-Haignere & Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-12-03 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/svnh/model-matching/Stimuli_from_Model-Matching_Experiment.html">http://mcdermottlab.mit.edu/svnh/model-matching/Stimuli_from_Model-Matching_Experiment.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/sensory.html">sensory</a>,&nbsp; 
                                        <a href="/tag/auditory-cortex.html">auditory-cortex</a>,&nbsp; 
                                        <a href="/tag/neuroscience.html">neuroscience</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/texture-time-averaging.html" rel="bookmark" title="Permalink to Texture-Time Averaging">Texture-Time Averaging</a></h1>
		<h4> Audio files showing the adaptive and selective time-averaging of auditory scenes. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/richard-mcwalter-josh-mcdermott.html">Richard McWalter & Josh McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-05-07 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/textint.html">http://mcdermottlab.mit.edu/textint.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/perception.html">perception</a>,&nbsp; 
                                        <a href="/tag/sensory-input.html">sensory-input</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/schema-learning.html" rel="bookmark" title="Permalink to Schema Learning for the Cocktail Party Problem">Schema Learning for the Cocktail Party Problem</a></h1>
		<h4> The cocktail party problem requires listeners to infer individual sound </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/kevin-jp-woods-josh-h-mcdermott.html">Kevin J.P. Woods & Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-04-03 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/schema_learning/index.html">http://mcdermottlab.mit.edu/schema_learning/index.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/sound-sources.html">sound-sources</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/schema.html">schema</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/reverberation.html" rel="bookmark" title="Permalink to Reverberation">Reverberation</a></h1>
		<h4> A large-scale statistical analysis of real-world acoustics, revealing strong regularities of reverberation in natural scenes. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/james-traer-josh-h-mcdermott.html">James Traer & Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2016-11-29 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/Reverb/ReverbSummary.html">http://mcdermottlab.mit.edu/Reverb/ReverbSummary.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/sound.html">sound</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/periphery.html">periphery</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/AUX.html" rel="bookmark" title="Permalink to AUditory syntaX (AUX)">AUditory syntaX (AUX)</a></h1>
		<h4> AUX (AUditory syntaX) is a scripting syntax specifically designed to describe and process auditory signals. In a nutshell, it consists of 1) functions to create and process sound signals, 2) operators particularly relevant to sounds, and 3) usual mathematic operations that you may find in any programming language. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/bomjun-j-kwon.html">Bomjun J. Kwon</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2016-07-20 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://auditorypro.com/download/aux/">http://auditorypro.com/download/aux/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/speech.html">speech</a>,&nbsp; 
                                        <a href="/tag/language.html">language</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/psychoacoustic.html">psychoacoustic</a>,&nbsp; 
                                        <a href="/tag/word-recognition.html">word-recognition</a>,&nbsp; 
                                        <a href="/tag/experiment.html">experiment</a>,&nbsp; 
                                        <a href="/tag/education.html">education</a>,&nbsp; 
                                        <a href="/tag/programming.html">programming</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/attentive-tracking.html" rel="bookmark" title="Permalink to Attentive Tracking of Sound Sources">Attentive Tracking of Sound Sources</a></h1>
		<h4> Humans track sound sources through feature space with a movable focus of attention. Attentive tracking aids segregation of similar sound sources. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/kevin-jp-woods-josh-h-mcdermott.html">Kevin J.P. Woods & Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2015-08-31 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/attentive_tracking/index.html">http://mcdermottlab.mit.edu/attentive_tracking/index.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/sound-source.html">sound-source</a>,&nbsp; 
                                        <a href="/tag/speech.html">speech</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/pitch-localizer.html" rel="bookmark" title="Permalink to Efficient Pitch Localizer">Efficient Pitch Localizer</a></h1>
		<h4> The Efficient Pitch Localizer stimuli are designed to effectively localize pitch regions and to work well with Sensimetrics Earphones. There are two types of stimuli: harmonic tones and frequency-matched gaussian noise. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/sam-norman-haignere.html">Sam Norman-Haignere</a>,&nbsp;
                                        <a class="url fn" href="/author/nancy-kanwisher.html">Nancy Kanwisher</a>,&nbsp;
                                        <a class="url fn" href="/author/josh-mcdermott.html">Josh McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2013-12-11 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://web.mit.edu/svnh/www/Resolvability/Efficient_Pitch_Localizer.html">http://web.mit.edu/svnh/www/Resolvability/Efficient_Pitch_Localizer.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/pitch.html">pitch</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/harmonics.html">harmonics</a>,&nbsp; 
                                        <a href="/tag/tonotopy.html">tonotopy</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/texture.html" rel="bookmark" title="Permalink to Auditory Textures">Auditory Textures</a></h1>
		<h4> Texture properties could be captured by summary statistics that are time-averages of acoustic measurements. We have explored the hypothesis that the auditory system represents textures in this way, with statistics of the measurements made in the peripheral auditory system. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/josh-h-mcdermott-eero-p-simoncelli.html">Josh H. McDermott & Eero P. Simoncelli</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2013-02-24 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/texture_examples/index.html">http://mcdermottlab.mit.edu/texture_examples/index.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/sound.html">sound</a>,&nbsp; 
                                        <a href="/tag/perception.html">perception</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/source-repetition.html" rel="bookmark" title="Permalink to Source Repetition Stimuli">Source Repetition Stimuli</a></h1>
		<h4> We used a simple generative model to synthesize novel sounds with naturalistic properties. We found that such sounds could be segregated and identified if they occurred more than once across different mixtures, even when the same sounds were impossible to segregate in single mixtures. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/josh-h-mcdermott.html">Josh H. McDermott</a>,&nbsp;
                                        <a class="url fn" href="/author/david-wrobleski.html">David Wrobleski</a>,&nbsp;
                                        <a class="url fn" href="/author/andrew-j-oxenham.html">Andrew J. Oxenham</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2011-01-18 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/source_repetition/index.html">http://mcdermottlab.mit.edu/source_repetition/index.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/speech.html">speech</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/sound-sources.html">sound-sources</a>,&nbsp; 
                                        <a href="/tag/acoustics.html">acoustics</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/music-scrambling.html" rel="bookmark" title="Permalink to Music Scrambling">Music Scrambling</a></h1>
		<h4> Example stimuli of music scrambled by pitch, rhythm, and both. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/sam-norman-haignere.html">Sam Norman-Haignere</a>,&nbsp;
                                        <a class="url fn" href="/author/nancy-kanwisher.html">Nancy Kanwisher</a>,&nbsp;
                                        <a class="url fn" href="/author/josh-h-mcdermott.html">Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2010-12-31 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/music_scrambling/index.html">http://mcdermottlab.mit.edu/music_scrambling/index.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/pitch.html">pitch</a>,&nbsp; 
                                        <a href="/tag/harmonics.html">harmonics</a>,&nbsp; 
                                        <a href="/tag/frequency.html">frequency</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/cocktail-party.html" rel="bookmark" title="Permalink to Cocktail Party Problem">Cocktail Party Problem</a></h1>
		<h4> The cocktail party problem is the task of hearing a sound of interest, often a speech signal, in a complex auditory setting. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/josh-h-mcdermott.html">Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2009-10-05 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/cocktail_examples/index.html">http://mcdermottlab.mit.edu/cocktail_examples/index.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/speech.html">speech</a>,&nbsp; 
                                        <a href="/tag/noise.html">noise</a>,&nbsp; 
                                        <a href="/tag/frequency.html">frequency</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/sound-contour.html" rel="bookmark" title="Permalink to Sound Contour Demos">Sound Contour Demos</a></h1>
		<h4> A study of relative pitch - the relationships between the pitch of successive sounds - to determine whether properties of relative pitch would generalize to other auditory dimensions. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/josh-h-mcdermott.html">Josh H. McDermott</a>,&nbsp;
                                        <a class="url fn" href="/author/andriana-j.html">Andriana J.</a>,&nbsp;
                                        <a class="url fn" href="/author/andrew-j-oxenham.html">Andrew J. Oxenham</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2008-05-09 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/sound_contour_demos/sound_contour_demos.html">http://mcdermottlab.mit.edu/sound_contour_demos/sound_contour_demos.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/frequency.html">frequency</a>,&nbsp; 
                                        <a href="/tag/pitch.html">pitch</a>,&nbsp; 
                                        <a href="/tag/harmony.html">harmony</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/spectral-completion.html" rel="bookmark" title="Permalink to Spectral Completion of Paritally Masked Sounds">Spectral Completion of Paritally Masked Sounds</a></h1>
		<h4> The auditory system uses the audible portions of an object's spectrum to infer the portions that are likely to have been masked, and that are thus not veridically present in the input to the auditory system. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/josh-h-mcdermott-andrew-j-oxenham.html">Josh H. McDermott & Andrew J. Oxenham</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2008-02-15 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/spec_comp_demos/spec_comp_page1.html">http://mcdermottlab.mit.edu/spec_comp_demos/spec_comp_page1.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/sound-sources.html">sound-sources</a>,&nbsp; 
                                        <a href="/tag/masking.html">masking</a>,&nbsp; 
                                        <a href="/tag/perception.html">perception</a> 
        </div>

        <br>


<!--  -->
<p class="paginator">
    <!--  -->
    Page 1 / 1
    <!--  -->
</p>
<!--  -->
  
        <footer id="contentinfo" class="body">
                <!-- <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address>

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p> -->
        </footer><!-- /#contentinfo -->

	<script src="https://files.stork-search.net/releases/v1.5.0/stork.js"></script>
	<script>
		stork.register("sitesearch", "/search-index.st")
	</script>
</body>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-8XWFESHEMV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());  gtag('config', 'G-8XWFESHEMV');
</script>
  
</html>