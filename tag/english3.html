<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="generator" content="Pelican" />
        <title>SLRB - English</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <link rel="icon" type="image/x-icon" href="https://www.slrb.net/theme/images/icons/slrb.svg">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
	<script>
    		$(function(){
        		$('a').each(function(){
            			if ($(this).prop('href') == window.location.href) {
                			$(this).addClass('active'); $(this).parents('li').addClass('active');
            			}
        		});
    		});
	</script>

	<link rel="stylesheet" href="https://files.stork-search.net/basic.css" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <div><img src="https://www.slrb.net/theme/images/icons/slrb.svg" height="30px"/>&nbsp;&nbsp;&nbsp;<h1>Speech and Language Resource Bank</h1></div>
                <div class="topnav">
                                <a href="/pages/home.html">home</a>
                                <a href="/pages/search.html">search</a>
                                <a href="/category/analysis.html">analysis</a>
                                <a href="/category/data.html">data</a>
                                <a href="/category/education.html">education</a>
                                <a href="/category/experimentation.html">experimentation</a>
		</div>
        </header>
	<br>

<div class="paginator">
    <a href="/tag/english.html"><img src="https://www.slrb.net/theme/images/icons/arrow-left-stop.png" alt="|<" style="vertical-align:middle;width:32px;height:32px;"></a>
    &nbsp; 
    <a href="/tag/english2.html"><img src="https://www.slrb.net/theme/images/icons/arrow-left.png" alt="<" style="vertical-align:middle;width:32px;height:32px;"></a>
    &nbsp; Page 3 / 10 &nbsp; 
    <a href="/tag/english4.html"><img src="https://www.slrb.net/theme/images/icons/arrow-right.png" alt=">" style="vertical-align:middle;width:32px;height:32px;"></a>
    &nbsp; 
    <a href="/tag/english10.html"><img src="https://www.slrb.net/theme/images/icons/arrow-right-stop.png" alt=">|" style="vertical-align:middle;width:32px;height:32px;"></a>
</div>    
    <br>
	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/ratcave.html" rel="bookmark" title="Permalink to Ratcave">Ratcave</a></h1>
		<h4> A free, open source Python 3D graphics library called Ratcave that extends existing Python psychology stimulus software by allowing scientists to load, display, and transform 3D stimuli created in 3D modeling software. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/nicholas-a-del-grosso-anton-sirota.html">Nicholas A. Del Grosso & Anton Sirota</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-12-18 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://github.com/ratcave/ratcave">https://github.com/ratcave/ratcave</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/psychology.html">psychology</a>,&nbsp; 
                                        <a href="/tag/python.html">Python</a>,&nbsp; 
                                        <a href="/tag/experiment.html">experiment</a>,&nbsp; 
                                        <a href="/tag/software.html">software</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/BITTSy.html" rel="bookmark" title="Permalink to BITTSy: Behavioral Infant & Toddler Testing System">BITTSy: Behavioral Infant & Toddler Testing System</a></h1>
		<h4> BITTSy is capable of running key infant behavioral testing paradigms, including Headturn Preference Procedure [HPP], Preferential Looking, and Visual Fixation/Habituation, through the same interface. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/rochelle-newman.html">Rochelle Newman</a>,&nbsp;
                                        <a class="url fn" href="/author/emily-shroads.html">Emily Shroads</a>,&nbsp;
                                        <a class="url fn" href="/author/elizabeth-johnson.html">Elizabeth Johnson</a>,&nbsp;
                                        <a class="url fn" href="/author/ruth-tincoff.html">Ruth Tincoff</a>,&nbsp;
                                        <a class="url fn" href="/author/kris-onishi.html">Kris Onishi</a>,&nbsp;
                                        <a class="url fn" href="/author/giovanna-morini.html">Giovanna Morini</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-09-04 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://langdev.umd.edu/bittsy/">http://langdev.umd.edu/bittsy/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/child-development.html">child-development</a>,&nbsp; 
                                        <a href="/tag/software.html">software</a>,&nbsp; 
                                        <a href="/tag/experiment.html">experiment</a>,&nbsp; 
                                        <a href="/tag/behavior.html">behavior</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/IRQ.html" rel="bookmark" title="Permalink to The Internal Representations Questionnaire">The Internal Representations Questionnaire</a></h1>
		<h4> The Internal Representations Questionnaire (IRQ) is a measure designed to quantify the subjective format of thought. There are four factors in the questionnaire namely; visual imagery, propensity to verbalize, representational manipulation and orthographic imagery. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/hettie-roebuck.html">Hettie Roebuck</a>,&nbsp;
                                        <a class="url fn" href="/author/pierce-edmiston.html">Pierce Edmiston</a>,&nbsp;
                                        <a class="url fn" href="/author/gary-lupyan.html">Gary Lupyan</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-07-14 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://osf.io/8rdzh/">https://osf.io/8rdzh/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/psychology.html">psychology</a>,&nbsp; 
                                        <a href="/tag/cognition.html">cognition</a>,&nbsp; 
                                        <a href="/tag/questionnaire.html">questionnaire</a>,&nbsp; 
                                        <a href="/tag/language.html">language</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/ModelTalker.html" rel="bookmark" title="Permalink to The Model Talker System">The Model Talker System</a></h1>
		<h4> The ModelTalker System converts plain English text to speech. It uses recorded speech (either from a prospective SGD user or from a voice donor chosen by or for the SGD user) to create a unique synthetic voice. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/h-timothy-bunnell.html">H. Timothy Bunnell</a>,&nbsp;
                                        <a class="url fn" href="/author/jason-lilley.html">Jason Lilley</a>,&nbsp;
                                        <a class="url fn" href="/author/matthew-buzzell.html">Matthew Buzzell</a>,&nbsp;
                                        <a class="url fn" href="/author/maxwell-schmid.html">Maxwell Schmid</a>,&nbsp;
                                        <a class="url fn" href="/author/bill-moyers.html">Bill Moyers</a>,&nbsp;
                                        <a class="url fn" href="/author/derek-freer.html">Derek Freer</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-06-25 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://www.modeltalker.org/">https://www.modeltalker.org/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/language.html">language</a>,&nbsp; 
                                        <a href="/tag/communication.html">communication</a>,&nbsp; 
                                        <a href="/tag/speech.html">speech</a>,&nbsp; 
                                        <a href="/tag/phonology.html">phonology</a>,&nbsp; 
                                        <a href="/tag/morphology.html">morphology</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/spatial-orientation.html" rel="bookmark" title="Permalink to Spatial Orientation Test">Spatial Orientation Test</a></h1>
		<h4> On each trial of the SOT, people are shown an array of objects; they have to imagine being located at one object, facing a second object (the orienting cue). They must indicate the direction of a third object (the target object) by drawing a line from the center of the circle in the direction believed to be correct. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/alinda-friedman.html">Alinda Friedman</a>,&nbsp;
                                        <a class="url fn" href="/author/alexander-paul-boone.html">Alexander Paul Boone</a>,&nbsp;
                                        <a class="url fn" href="/author/bernd-kohler.html">Bernd Kohler</a>,&nbsp;
                                        <a class="url fn" href="/author/mary-hegarty.html">Mary Hegarty</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-05-26 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://osf.io/wq3kd/">https://osf.io/wq3kd/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/psychology.html">psychology</a>,&nbsp; 
                                        <a href="/tag/experiment.html">experiment</a>,&nbsp; 
                                        <a href="/tag/spatial-ability.html">spatial-ability</a>,&nbsp; 
                                        <a href="/tag/perspective.html">perspective</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/ASL-CDI.html" rel="bookmark" title="Permalink to ASL-CDI 2.0">ASL-CDI 2.0</a></h1>
		<h4> The ASL-CDI 2.0 is an updated American Sign Language adaptation of the MacArthur Bates Communicative Development Inventory. It is an assessment of early vocabulary knowledge in children learning American Sign Language. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/naomi-caselli.html">Naomi Caselli</a>,&nbsp;
                                        <a class="url fn" href="/author/jennie-pyers.html">Jennie Pyers</a>,&nbsp;
                                        <a class="url fn" href="/author/amy-lieberman.html">Amy Lieberman</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-04-30 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://www.aslcdi.org/">https://www.aslcdi.org/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/vocabulary.html">vocabulary</a>,&nbsp; 
                                        <a href="/tag/language-development.html">language-development</a>,&nbsp; 
                                        <a href="/tag/communication.html">communication</a>,&nbsp; 
                                        <a href="/tag/american-sign-language.html">American-Sign-Language</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/ERT.html" rel="bookmark" title="Permalink to The Emotion Recognition Task">The Emotion Recognition Task</a></h1>
		<h4> The ERT is a computerized task to assess the perception of facial expressions. The task presents morphed facial expressions that gradually increase in intensity. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/barbara-montagne.html">Barbara Montagne</a>,&nbsp;
                                        <a class="url fn" href="/author/roy-kessels.html">Roy Kessels</a>,&nbsp;
                                        <a class="url fn" href="/author/david-perrett.html">David Perrett</a>,&nbsp;
                                        <a class="url fn" href="/author/edward-de-haan.html">Edward de Haan</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-04-30 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://www.emotionrecognitiontask.com/">https://www.emotionrecognitiontask.com/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/emotion.html">emotion</a>,&nbsp; 
                                        <a href="/tag/psychology.html">psychology</a>,&nbsp; 
                                        <a href="/tag/neuropsychology.html">neuropsychology</a>,&nbsp; 
                                        <a href="/tag/cognition.html">cognition</a>,&nbsp; 
                                        <a href="/tag/dutch.html">Dutch</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a>,&nbsp; 
                                        <a href="/tag/german.html">German</a>,&nbsp; 
                                        <a href="/tag/french.html">French</a>,&nbsp; 
                                        <a href="/tag/spanish.html">Spanish</a>,&nbsp; 
                                        <a href="/tag/finnish.html">Finnish</a>,&nbsp; 
                                        <a href="/tag/italian.html">Italian</a>,&nbsp; 
                                        <a href="/tag/russian.html">Russian</a>,&nbsp; 
                                        <a href="/tag/lithuanian.html">Lithuanian</a>,&nbsp; 
                                        <a href="/tag/greek.html">Greek</a>,&nbsp; 
                                        <a href="/tag/portuguese.html">Portuguese</a>,&nbsp; 
                                        <a href="/tag/turkish.html">Turkish</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/storybook-apps.html" rel="bookmark" title="Permalink to VL2 Storybook Apps">VL2 Storybook Apps</a></h1>
		<h4> Stories told in sign language by fluent Deaf storytellers. Easy & accessible navigation designed for children. Page-by-page sign language videos supporting the printed sentences text. Rich interactive narrative with direct English to ASL vocabulary video translation. 120+ new vocabulary words with each app. Parents can learn new ASL signs along with their child. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/motion-light-lab.html">Motion Light Lab</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-04-30 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://vl2storybookapps.com/digital-library">https://vl2storybookapps.com/digital-library</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/language.html">language</a>,&nbsp; 
                                        <a href="/tag/education.html">education</a>,&nbsp; 
                                        <a href="/tag/bilingual.html">bilingual</a>,&nbsp; 
                                        <a href="/tag/american-sign-language.html">American-Sign-Language</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/COCA.html" rel="bookmark" title="Permalink to Corpus of Contemporary American English">Corpus of Contemporary American English</a></h1>
		<h4> The Corpus of Contemporary American English (COCA) is the only large, genre-balanced corpus of American English. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/mark-davies.html">Mark Davies</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-03-30 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://www.english-corpora.org/coca/">https://www.english-corpora.org/coca/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/english.html">English</a>,&nbsp; 
                                        <a href="/tag/corpora.html">corpora</a>,&nbsp; 
                                        <a href="/tag/linguistics.html">linguistics</a>,&nbsp; 
                                        <a href="/tag/frequency-data.html">frequency-data</a>,&nbsp; 
                                        <a href="/tag/word-form.html">word-form</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/ArtLex-en.html" rel="bookmark" title="Permalink to ArtLex-en: Acoustic and EMA data on thousands of words and syllables spoken by a single speaker of American English">ArtLex-en: Acoustic and EMA data on thousands of words and syllables spoken by a single speaker of American English</a></h1>
		<h4> EMA and acoustic data on over 20,000 words and approximately 2,000 controlled syllables spoken in isolation by a single speaker of Midwestern American English. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/charles-redmon.html">Charles Redmon</a>,&nbsp;
                                        <a class="url fn" href="/author/seulgi-shin.html">Seulgi Shin</a>,&nbsp;
                                        <a class="url fn" href="/author/panying-rong.html">Panying Rong</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-03-10 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://gitlab.com/chredmon/ku-artlex_eng1">https://gitlab.com/chredmon/ku-artlex_eng1</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/electromagnetic-articulography.html">electromagnetic-articulography</a>,&nbsp; 
                                        <a href="/tag/acoustics.html">acoustics</a>,&nbsp; 
                                        <a href="/tag/lexicon.html">lexicon</a>,&nbsp; 
                                        <a href="/tag/english.html">english</a> 
        </div>

        <br>


<div class="paginator">
    <a href="/tag/english.html"><img src="https://www.slrb.net/theme/images/icons/arrow-left-stop.png" alt="|<" style="vertical-align:middle;width:32px;height:32px;"></a>
    &nbsp; 
    <a href="/tag/english2.html"><img src="https://www.slrb.net/theme/images/icons/arrow-left.png" alt="<" style="vertical-align:middle;width:32px;height:32px;"></a>
    &nbsp; Page 3 / 10 &nbsp; 
    <a href="/tag/english4.html"><img src="https://www.slrb.net/theme/images/icons/arrow-right.png" alt=">" style="vertical-align:middle;width:32px;height:32px;"></a>
    &nbsp; 
    <a href="/tag/english10.html"><img src="https://www.slrb.net/theme/images/icons/arrow-right-stop.png" alt=">|" style="vertical-align:middle;width:32px;height:32px;"></a>
</div>
  
        <footer id="contentinfo" class="body">
                <!-- <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address>

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p> -->
        </footer><!-- /#contentinfo -->

	<script src="https://files.stork-search.net/releases/v1.5.0/stork.js"></script>
	<script>
		stork.register("sitesearch", "/search-index.st")
	</script>
</body>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-8XWFESHEMV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());  gtag('config', 'G-8XWFESHEMV');
</script>
  
</html>