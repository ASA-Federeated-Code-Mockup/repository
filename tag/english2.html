<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="generator" content="Pelican" />
        <title>SLRB - English</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <link rel="icon" href="https://slrb.net/theme/images/icons/slrb.svg" type="image/x-icon">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
	<script>
    		$(function(){
        		$('a').each(function(){
            			if ($(this).prop('href') == window.location.href) {
                			$(this).addClass('active'); $(this).parents('li').addClass('active');
            			}
        		});
    		});
	</script>

	<link rel="stylesheet" href="https://files.stork-search.net/basic.css" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <div><img src="https://www.slrb.net/theme/images/icons/slrb.svg" height="30px"/>&nbsp;&nbsp;&nbsp;<h1>Speech and Language Resource Bank</h1></div>
                <div class="topnav">
                                <a href="/pages/home.html">home</a>
                                <a href="/pages/search.html">search</a>
                                <a href="/category/analysis.html">analysis</a>
                                <a href="/category/data.html">data</a>
                                <a href="/category/education.html">education</a>
                                <a href="/category/experimentation.html">experimentation</a>
		</div>
        </header>
	<br>

<p class="paginator">
        <a href="/tag/english.html"><i class="fa-solid fa-arrow-left-to-line"></i></a>
        <a href="/tag/english.html"><i class="fa-arrow-circle-left"></i></a>
    Page 2 / 4
        <a href="/tag/english3.html"><i class="fa-arrow-circle-right"></i></a>
        <a href="/tag/english4.html"><i class="fa-solid fa-arrow-right-to-line"></i></a>
</p>
    
    <br>
	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/ERT.html" rel="bookmark" title="Permalink to The Emotion Recognition Task">The Emotion Recognition Task</a></h1>
		<h4> The ERT is a computerized task to assess the perception of facial expressions. The task presents morphed facial expressions that gradually increase in intensity. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/barbara-montagne.html">Barbara Montagne</a>,&nbsp;
                                        <a class="url fn" href="/author/roy-kessels.html">Roy Kessels</a>,&nbsp;
                                        <a class="url fn" href="/author/david-perrett.html">David Perrett</a>,&nbsp;
                                        <a class="url fn" href="/author/edward-de-haan.html">Edward de Haan</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-10-05 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://www.emotionrecognitiontask.com/">https://www.emotionrecognitiontask.com/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/emotion.html">emotion</a>,&nbsp; 
                                        <a href="/tag/psychology.html">psychology</a>,&nbsp; 
                                        <a href="/tag/neuropsychology.html">neuropsychology</a>,&nbsp; 
                                        <a href="/tag/cognition.html">cognition</a>,&nbsp; 
                                        <a href="/tag/dutch.html">Dutch</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a>,&nbsp; 
                                        <a href="/tag/german.html">German</a>,&nbsp; 
                                        <a href="/tag/french.html">French</a>,&nbsp; 
                                        <a href="/tag/spanish.html">Spanish</a>,&nbsp; 
                                        <a href="/tag/finnish.html">Finnish</a>,&nbsp; 
                                        <a href="/tag/italian.html">Italian</a>,&nbsp; 
                                        <a href="/tag/russian.html">Russian</a>,&nbsp; 
                                        <a href="/tag/lithuanian.html">Lithuanian</a>,&nbsp; 
                                        <a href="/tag/greek.html">Greek</a>,&nbsp; 
                                        <a href="/tag/portuguese.html">Portuguese</a>,&nbsp; 
                                        <a href="/tag/turkish.html">Turkish</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/SemDis.html" rel="bookmark" title="Permalink to SemDis">SemDis</a></h1>
		<h4> SemDis uses advances in natural language processing to automatically determine how closely associated texts are to each other. Higher SemDis scores indicate two texts are less related, that is, they are more distantly related ideas or concepts. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/dan-johnson-roger-beaty.html">Dan Johnson & Roger Beaty</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-10-05 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://semdis.wlu.psu.edu/">http://semdis.wlu.psu.edu/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/language.html">language</a>,&nbsp; 
                                        <a href="/tag/semantics.html">semantics</a>,&nbsp; 
                                        <a href="/tag/word-recognition.html">word-recognition</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/storybook-apps.html" rel="bookmark" title="Permalink to VL2 Storybook Apps">VL2 Storybook Apps</a></h1>
		<h4> Stories told in sign language by fluent Deaf storytellers. Easy & accessible navigation designed for children. Page-by-page sign language videos supporting the printed sentences text. Rich interactive narrative with direct English to ASL vocabulary video translation. 120+ new vocabulary words with each app. Parents can learn new ASL signs along with their child. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/motion-light-lab.html">Motion Light Lab</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-10-05 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://vl2storybookapps.com/digital-library">https://vl2storybookapps.com/digital-library</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/language.html">language</a>,&nbsp; 
                                        <a href="/tag/education.html">education</a>,&nbsp; 
                                        <a href="/tag/bilingual.html">bilingual</a>,&nbsp; 
                                        <a href="/tag/american-sign-language.html">American-Sign-Language</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/BITTSy.html" rel="bookmark" title="Permalink to BITTSy: Behavioral Infant & Toddler Testing System">BITTSy: Behavioral Infant & Toddler Testing System</a></h1>
		<h4> BITTSy is capable of running key infant behavioral testing paradigms, including Headturn Preference Procedure [HPP], Preferential Looking, and Visual Fixation/Habituation, through the same interface. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/rochelle-newman.html">Rochelle Newman</a>,&nbsp;
                                        <a class="url fn" href="/author/emily-shroads.html">Emily Shroads</a>,&nbsp;
                                        <a class="url fn" href="/author/elizabeth-johnson.html">Elizabeth Johnson</a>,&nbsp;
                                        <a class="url fn" href="/author/ruth-tincoff.html">Ruth Tincoff</a>,&nbsp;
                                        <a class="url fn" href="/author/kris-onishi.html">Kris Onishi</a>,&nbsp;
                                        <a class="url fn" href="/author/giovanna-morini.html">Giovanna Morini</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-09-04 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://langdev.umd.edu/bittsy/">http://langdev.umd.edu/bittsy/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/child-development.html">child-development</a>,&nbsp; 
                                        <a href="/tag/software.html">software</a>,&nbsp; 
                                        <a href="/tag/experiment.html">experiment</a>,&nbsp; 
                                        <a href="/tag/behavior.html">behavior</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/IRQ.html" rel="bookmark" title="Permalink to The Internal Representations Questionnaire">The Internal Representations Questionnaire</a></h1>
		<h4> The Internal Representations Questionnaire (IRQ) is a measure designed to quantify the subjective format of thought. There are four factors in the questionnaire namely; visual imagery, propensity to verbalize, representational manipulation and orthographic imagery. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/hettie-roebuck.html">Hettie Roebuck</a>,&nbsp;
                                        <a class="url fn" href="/author/pierce-edmiston.html">Pierce Edmiston</a>,&nbsp;
                                        <a class="url fn" href="/author/gary-lupyan.html">Gary Lupyan</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-07-14 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://osf.io/8rdzh/">https://osf.io/8rdzh/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/psychology.html">psychology</a>,&nbsp; 
                                        <a href="/tag/cognition.html">cognition</a>,&nbsp; 
                                        <a href="/tag/questionnaire.html">questionnaire</a>,&nbsp; 
                                        <a href="/tag/language.html">language</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/ModelTalker.html" rel="bookmark" title="Permalink to The Model Talker System">The Model Talker System</a></h1>
		<h4> The ModelTalker System converts plain English text to speech. It uses recorded speech (either from a prospective SGD user or from a voice donor chosen by or for the SGD user) to create a unique synthetic voice. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/h-timothy-bunnell.html">H. Timothy Bunnell</a>,&nbsp;
                                        <a class="url fn" href="/author/jason-lilley.html">Jason Lilley</a>,&nbsp;
                                        <a class="url fn" href="/author/matthew-buzzell.html">Matthew Buzzell</a>,&nbsp;
                                        <a class="url fn" href="/author/maxwell-schmid.html">Maxwell Schmid</a>,&nbsp;
                                        <a class="url fn" href="/author/bill-moyers.html">Bill Moyers</a>,&nbsp;
                                        <a class="url fn" href="/author/derek-freer.html">Derek Freer</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-06-25 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://www.modeltalker.org/">https://www.modeltalker.org/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/language.html">language</a>,&nbsp; 
                                        <a href="/tag/communication.html">communication</a>,&nbsp; 
                                        <a href="/tag/speech.html">speech</a>,&nbsp; 
                                        <a href="/tag/phonology.html">phonology</a>,&nbsp; 
                                        <a href="/tag/morphology.html">morphology</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/spatial-orientation.html" rel="bookmark" title="Permalink to Spatial Orientation Test">Spatial Orientation Test</a></h1>
		<h4> On each trial of the SOT, people are shown an array of objects; they have to imagine being located at one object, facing a second object (the orienting cue). They must indicate the direction of a third object (the target object) by drawing a line from the center of the circle in the direction believed to be correct. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/alinda-friedman.html">Alinda Friedman</a>,&nbsp;
                                        <a class="url fn" href="/author/alexander-paul-boone.html">Alexander Paul Boone</a>,&nbsp;
                                        <a class="url fn" href="/author/bernd-kohler.html">Bernd Kohler</a>,&nbsp;
                                        <a class="url fn" href="/author/mary-hegarty.html">Mary Hegarty</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-05-26 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://osf.io/wq3kd/">https://osf.io/wq3kd/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/psychology.html">psychology</a>,&nbsp; 
                                        <a href="/tag/experiment.html">experiment</a>,&nbsp; 
                                        <a href="/tag/spatial-ability.html">spatial-ability</a>,&nbsp; 
                                        <a href="/tag/perspective.html">perspective</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/COCA.html" rel="bookmark" title="Permalink to Corpus of Contemporary American English">Corpus of Contemporary American English</a></h1>
		<h4> The Corpus of Contemporary American English (COCA) is the only large, genre-balanced corpus of American English. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/mark-davies.html">Mark Davies</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2020-03-05 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://www.english-corpora.org/coca/">https://www.english-corpora.org/coca/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/english.html">English</a>,&nbsp; 
                                        <a href="/tag/corpora.html">corpora</a>,&nbsp; 
                                        <a href="/tag/linguistics.html">linguistics</a>,&nbsp; 
                                        <a href="/tag/frequency-data.html">frequency-data</a>,&nbsp; 
                                        <a href="/tag/word-form.html">word-form</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/CMUSphinx.html" rel="bookmark" title="Permalink to CMUSphinx4">CMUSphinx4</a></h1>
		<h4> CMU Sphinx is a set of speech recognition development libraries and tools that can be linked in to speech-enable applications. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/evandro-gouvea.html">Evandro Gouvea</a>,&nbsp;
                                        <a class="url fn" href="/author/peter-gorniak.html">Peter Gorniak</a>,&nbsp;
                                        <a class="url fn" href="/author/philip-kwok.html">Philip Kwok</a>,&nbsp;
                                        <a class="url fn" href="/author/paul-lamere.html">Paul Lamere</a>,&nbsp;
                                        <a class="url fn" href="/author/beth-logan.html">Beth Logan</a>,&nbsp;
                                        <a class="url fn" href="/author/pedro-moreno.html">Pedro Moreno</a>,&nbsp;
                                        <a class="url fn" href="/author/bhiksha-raj.html">Bhiksha Raj</a>,&nbsp;
                                        <a class="url fn" href="/author/mosur-ravishankar.html">Mosur Ravishankar</a>,&nbsp;
                                        <a class="url fn" href="/author/bent-schmidt-nielsen.html">Bent Schmidt-Nielsen</a>,&nbsp;
                                        <a class="url fn" href="/author/rita-singh.html">Rita Singh</a>,&nbsp;
                                        <a class="url fn" href="/author/jm-van-thong.html">JM Van Thong</a>,&nbsp;
                                        <a class="url fn" href="/author/willie-walker.html">Willie Walker</a>,&nbsp;
                                        <a class="url fn" href="/author/manfred-warmuth.html">Manfred Warmuth</a>,&nbsp;
                                        <a class="url fn" href="/author/joe-woelfel.html">Joe Woelfel</a>,&nbsp;
                                        <a class="url fn" href="/author/peter-wolf.html">Peter Wolf</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2019-10-23 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://cmusphinx.github.io/">https://cmusphinx.github.io/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/speech.html">speech</a>,&nbsp; 
                                        <a href="/tag/programming.html">programming</a>,&nbsp; 
                                        <a href="/tag/java.html">Java</a>,&nbsp; 
                                        <a href="/tag/experiment.html">experiment</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a>,&nbsp; 
                                        <a href="/tag/french.html">French</a>,&nbsp; 
                                        <a href="/tag/mandarin.html">Mandarin</a>,&nbsp; 
                                        <a href="/tag/german.html">German</a>,&nbsp; 
                                        <a href="/tag/dutch.html">Dutch</a>,&nbsp; 
                                        <a href="/tag/russian.html">Russian</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/AELP.html" rel="bookmark" title="Permalink to Auditory English Lexicon Project">Auditory English Lexicon Project</a></h1>
		<h4> The Auditory English Lexicon Project (AELP) is a multi-talker, multi-region psycholinguistic database of 10,170 spoken words and 10,170 spoken nonwords. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/winston-d-goh.html">Winston D. Goh</a>,&nbsp;
                                        <a class="url fn" href="/author/melvin-j-yap.html">Melvin J. Yap</a>,&nbsp;
                                        <a class="url fn" href="/author/qian-wen-chee.html">Qian Wen Chee</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2019-10-05 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://inetapps.nus.edu.sg/aelp/">https://inetapps.nus.edu.sg/aelp/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/psycholinguistics.html">psycholinguistics</a>,&nbsp; 
                                        <a href="/tag/database.html">database</a>,&nbsp; 
                                        <a href="/tag/lexicon.html">lexicon</a>,&nbsp; 
                                        <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/semantics.html">semantics</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/predpsych.html" rel="bookmark" title="Permalink to PredPsych">PredPsych</a></h1>
		<h4> PredPsych is a user-friendly toolbox based on machine learning predictive algorithms. It comprises of multiple functionalities for multivariate analyses of quantitative behavioral data based on machine learning models. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/atesh-koul.html">Atesh Koul</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2019-07-23 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://CRAN.R-project.org/package=PredPsych ; https://github.com/ateshkoul/PredPsych">https://CRAN.R-project.org/package=PredPsych ; https://github.com/ateshkoul/PredPsych</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/psychology.html">psychology</a>,&nbsp; 
                                        <a href="/tag/experiment.html">experiment</a>,&nbsp; 
                                        <a href="/tag/programming.html">programming</a>,&nbsp; 
                                        <a href="/tag/neuroscience.html">neuroscience</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/NNDIE.html" rel="bookmark" title="Permalink to Normative Data on Dutch Idiomatic Expressions">Normative Data on Dutch Idiomatic Expressions</a></h1>
		<h4> Normative data of 374 Dutch idiomatic expressions by 390 native speakers. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/ferdy-hubers.html">Ferdy Hubers</a>,&nbsp;
                                        <a class="url fn" href="/author/catia-cucchiarini.html">Catia Cucchiarini</a>,&nbsp;
                                        <a class="url fn" href="/author/helmer-strik.html">Helmer Strik</a>,&nbsp;
                                        <a class="url fn" href="/author/ton-dijkstra.html">Ton Dijkstra</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2019-05-14 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://www.ru.nl/cls/publications/corpora/">https://www.ru.nl/cls/publications/corpora/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/language.html">language</a>,&nbsp; 
                                        <a href="/tag/speech.html">speech</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a>,&nbsp; 
                                        <a href="/tag/dutch.html">Dutch</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/model-match.html" rel="bookmark" title="Permalink to Model-Matched Sounds">Model-Matched Sounds</a></h1>
		<h4> Cochleograms and sound files are shown for example stimuli from the model-matching experiment. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/sam-v-norman-haignere-josh-h-mcdermott.html">Sam V. Norman-Haignere & Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-12-03 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/svnh/model-matching/Stimuli_from_Model-Matching_Experiment.html">http://mcdermottlab.mit.edu/svnh/model-matching/Stimuli_from_Model-Matching_Experiment.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/audition.html">audition</a>,&nbsp; 
                                        <a href="/tag/sensory.html">sensory</a>,&nbsp; 
                                        <a href="/tag/auditory-cortex.html">auditory-cortex</a>,&nbsp; 
                                        <a href="/tag/neuroscience.html">neuroscience</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/PBCM.html" rel="bookmark" title="Permalink to PBCM: Code-mixed Hindi-English corpus">PBCM: Code-mixed Hindi-English corpus</a></h1>
		<h4> A multispeaker code-mixed Hindi read-speech corpus. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/ayushi-pandey.html">Ayushi Pandey</a>,&nbsp;
                                        <a class="url fn" href="/author/brij-mohan-lal-srivastava.html">Brij Mohan Lal Srivastava</a>,&nbsp;
                                        <a class="url fn" href="/author/rohit-kumar.html">Rohit Kumar</a>,&nbsp;
                                        <a class="url fn" href="/author/bt-nellore.html">BT Nellore</a>,&nbsp;
                                        <a class="url fn" href="/author/ks-teja.html">KS Teja</a>,&nbsp;
                                        <a class="url fn" href="/author/sv-gangashetty.html">SV Gangashetty</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-10-05 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://brijmohan.github.io/publication/pbcm-lrec18/">https://brijmohan.github.io/publication/pbcm-lrec18/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/code-mixing.html">code-mixing</a>,&nbsp; 
                                        <a href="/tag/hindi-english.html">hindi-english</a>,&nbsp; 
                                        <a href="/tag/hindi.html">hindi</a>,&nbsp; 
                                        <a href="/tag/english.html">english</a>,&nbsp; 
                                        <a href="/tag/multi-speaker.html">multi-speaker</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/PC-PVT.html" rel="bookmark" title="Permalink to PC-PVT">PC-PVT</a></h1>
		<h4> A freely available system for PC-based simple visual reaction time testing that is analogous to the widely used psychomotor vigilance task (PVT). </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/jaques-reifman.html">Jaques Reifman</a>,&nbsp;
                                        <a class="url fn" href="/author/maxim-y-khitrov.html">Maxim Y. Khitrov</a>,&nbsp;
                                        <a class="url fn" href="/author/sridhar-ramakrishnan.html">Sridhar Ramakrishnan</a>,&nbsp;
                                        <a class="url fn" href="/author/kamal-kumar.html">Kamal Kumar</a>,&nbsp;
                                        <a class="url fn" href="/author/jianbo-liu.html">Jianbo Liu</a>,&nbsp;
                                        <a class="url fn" href="/author/srinivas-laxminarayan.html">Srinivas Laxminarayan</a>,&nbsp;
                                        <a class="url fn" href="/author/david-thorsley.html">David Thorsley</a>,&nbsp;
                                        <a class="url fn" href="/author/srinivasan-rajaraman.html">Srinivasan Rajaraman</a>,&nbsp;
                                        <a class="url fn" href="/author/nancy-j-wesensten.html">Nancy J. Wesensten</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-10-05 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://pcpvt.bhsai.org/pcpvt/register.xhtml">https://pcpvt.bhsai.org/pcpvt/register.xhtml</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/psychology.html">psychology</a>,&nbsp; 
                                        <a href="/tag/neuroscience.html">neuroscience</a>,&nbsp; 
                                        <a href="/tag/experiment.html">experiment</a>,&nbsp; 
                                        <a href="/tag/software.html">software</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/phonotactic-probability-calculator.html" rel="bookmark" title="Permalink to Phonotactic Probability Calculator">Phonotactic Probability Calculator</a></h1>
		<h4> Used to calculate the phonotactic probabilities of real English, Arabic, and Spanish words or specially constructed nonwords. Analyses can be performed on words or nonwords up to 17 phonemes in length. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/michael-vitevitch.html">Michael Vitevitch</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-10-05 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://calculator.ku.edu/phonotactic/about">https://calculator.ku.edu/phonotactic/about</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/phonotactic-probability.html">phonotactic-probability</a>,&nbsp; 
                                        <a href="/tag/psychology.html">psychology</a>,&nbsp; 
                                        <a href="/tag/phonetics.html">phonetics</a>,&nbsp; 
                                        <a href="/tag/language-processing.html">language-processing</a>,&nbsp; 
                                        <a href="/tag/arabic.html">Arabic</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a>,&nbsp; 
                                        <a href="/tag/spanish.html">Spanish</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/ACAD.html" rel="bookmark" title="Permalink to Arizona Child Acoustic Database">Arizona Child Acoustic Database</a></h1>
		<h4> The Arizona Child Acoustic Database is a longitudinal collection of audio samples from children between the ages of 2-7 years. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/kate-bunton-and-brad-story.html">Kate Bunton and Brad Story</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-06-20 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://repository.arizona.edu/handle/10150/316065">https://repository.arizona.edu/handle/10150/316065</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/language.html">language</a>,&nbsp; 
                                        <a href="/tag/speech-development.html">speech-development</a>,&nbsp; 
                                        <a href="/tag/acoustics.html">acoustics</a>,&nbsp; 
                                        <a href="/tag/linguistics.html">linguistics</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
			<span class="fa fa-check-circle"></span> documented&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/MALD.html" rel="bookmark" title="Permalink to Massive Auditory Lexical Decision database">Massive Auditory Lexical Decision database</a></h1>
		<h4> Behavioral data and stimuli from an auditory lexical decision megastudy (26,793 words and 9592 pseudowords) </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/benjamin-v-tucker.html">Benjamin V. Tucker</a>,&nbsp;
                                        <a class="url fn" href="/author/daniel-brenner.html">Daniel Brenner</a>,&nbsp;
                                        <a class="url fn" href="/author/d-kyle-danielson.html">D. Kyle Danielson</a>,&nbsp;
                                        <a class="url fn" href="/author/matthew-c-kelley.html">Matthew C. Kelley</a>,&nbsp;
                                        <a class="url fn" href="/author/filip-nenadic.html">Filip Nenadić</a>,&nbsp;
                                        <a class="url fn" href="/author/michelle-sims.html">Michelle Sims</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-06-18 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mald.artsrn.ualberta.ca/">http://mald.artsrn.ualberta.ca/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/auditory-lexical-decision.html">auditory-lexical-decision</a>,&nbsp; 
                                        <a href="/tag/frequency.html">frequency</a>,&nbsp; 
                                        <a href="/tag/phonological-neighbors.html">phonological-neighbors</a>,&nbsp; 
                                        <a href="/tag/english.html">english</a>,&nbsp; 
                                        <a href="/tag/audio-data.html">audio-data</a>,&nbsp; 
                                        <a href="/tag/behavioral-data.html">behavioral-data</a>,&nbsp; 
                                        <a href="/tag/spoken-word-recognition.html">spoken-word-recognition</a>,&nbsp; 
                                        <a href="/tag/megastudy.html">megastudy</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/inharmonic-speech-segregation.html" rel="bookmark" title="Permalink to Inharmonic Speech Segregation">Inharmonic Speech Segregation</a></h1>
		<h4> Inharmonic speech demos showing sound segregation. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/sara-popham.html">Sara Popham</a>,&nbsp;
                                        <a class="url fn" href="/author/dana-boebinger.html">Dana Boebinger</a>,&nbsp;
                                        <a class="url fn" href="/author/dan-p-w-ellis.html">Dan P. W. Ellis</a>,&nbsp;
                                        <a class="url fn" href="/author/hideki-kawahara.html">Hideki Kawahara</a>,&nbsp;
                                        <a class="url fn" href="/author/josh-h-mcdermott.html">Josh H. McDermott</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-05-29 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="http://mcdermottlab.mit.edu/inharmonic_speech_examples/index.html">http://mcdermottlab.mit.edu/inharmonic_speech_examples/index.html</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/sound-sources.html">sound-sources</a>,&nbsp; 
                                        <a href="/tag/brain.html">brain</a>,&nbsp; 
                                        <a href="/tag/frequency.html">frequency</a>,&nbsp; 
                                        <a href="/tag/harmonics.html">harmonics</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>

	<div class="note_header">
			<span class="fa fa-unlock-alt"></span> open&nbsp;&nbsp;
	</div>
        <div class="note">
	        <h1><a href="/BilingBank.html" rel="bookmark" title="Permalink to BilingBank">BilingBank</a></h1>
		<h4> BilingBank is a component of TalkBank dedicated to providing corpora for the study of multilingualism. </h4>
                <strong>Authors:</strong>&nbsp;                                          <a class="url fn" href="/author/brian-macwhinney.html">Brian MacWhinney</a>
 <br>
                <strong>Updated:</strong>&nbsp; 2018-05-04 <br>
            <!-- <br> -->
		<strong>Source:</strong>&nbsp; <a href="https://biling.talkbank.org/">https://biling.talkbank.org/</a><br>
                <strong>Keywords:</strong>&nbsp;                                         <a href="/tag/bilingual.html">bilingual</a>,&nbsp; 
                                        <a href="/tag/multilingualism.html">multilingualism</a>,&nbsp; 
                                        <a href="/tag/language.html">language</a>,&nbsp; 
                                        <a href="/tag/communication.html">communication</a>,&nbsp; 
                                        <a href="/tag/english.html">English</a> 
        </div>

        <br>


<p class="paginator">
        <a href="/tag/english.html"><i class="fa-solid fa-arrow-left-to-line"></i></a>
        <a href="/tag/english.html"><i class="fa-arrow-circle-left"></i></a>
    Page 2 / 4
        <a href="/tag/english3.html"><i class="fa-arrow-circle-right"></i></a>
        <a href="/tag/english4.html"><i class="fa-solid fa-arrow-right-to-line"></i></a>
</p>

  
        <footer id="contentinfo" class="body">
                <!-- <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address>

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p> -->
        </footer><!-- /#contentinfo -->

	<script src="https://files.stork-search.net/releases/v1.5.0/stork.js"></script>
	<script>
		stork.register("sitesearch", "/search-index.st")
	</script>
</body>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-8XWFESHEMV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());  gtag('config', 'G-8XWFESHEMV');
</script>
  
</html>