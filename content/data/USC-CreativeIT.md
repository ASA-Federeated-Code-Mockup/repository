title: USC CreativeIT database of multimodal dyadic interactions  
slug: USC-CreativeIT  
authors: Angeliki Metallinou, Zhaojun Yang, Chi-Chun Lee, Carlos Busso, Sharon Carnicke, Shrikanth Narayanan  
date: 2015-04-17  
source: https://sail.usc.edu/CreativeIT/  
type: multimodal-database  
languages: english  
tags: dyadic-interactions, speech, gestures, motion-capture, emotion, english  
open_access: yes  
license: https://sail.usc.edu/CreativeIT/Data_Release_Form_CreativeIT.pdf  
publications: Metallinou et al. (2016)  
citation: Metallinou, A., Yang, Z., Lee, C-C., Busso, C., Carnicke, S., & Narayanan, S. (2016). The USC CreativeIT database of multimodal dyadic interactions: From speech and full body motion capture to continuous emotional annotations. Language Resources and Evaluation, 50(3), 497-521.  
shortdesc: Data from 16 actors, male and female, during their affective dyadic interactions ranging from 2-10 minutes each, and two types of improvised interactions: 2 sentence exercises and paraphrases.  
summary: For each recording, we provide detailed audiovisual and text information, which consists of the audio and video of both interlocutors, the Motion Capture data of the full body of one of the interlocutors in each recording, the text transcriptions of the interaction. Also, for each actor-recording, we provide discrete and time-continuous annotations of dimensional emotion labels, from multiple annotators.  
<!--
documentation:   
tests:
coverage:
reviews:
-->
