title: IEMOCAP: The Interactive Emotional Dyadic Motion Capture Database  
slug: IEMOCAP  
authors: Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N. Chang, Sungbok Lee, Shrikanth Narayanan  
date: 2008-11-09  
source: https://sail.usc.edu/iemocap/  
type: mulitmodal-database  
languages: english  
tags: emotions, behavior, speech, gesture, motion-capture, english    
open_access: yes  
license: https://sail.usc.edu/iemocap/Data_Release_Form_IEMOCAP.pdf  
documentation: https://sail.usc.edu/iemocap/iemocap_info.htm  
publications: Busso et al. (2008)  
citation: Busso, C., Bulut, M., Lee, C. C., Kazemzadeh, A., Mower, E., Kim, S., Chang, J. N., Lee, S., & Narayanan, S. S. (2008). IEMOCAP: Interactive emotional dyadic motion capture database. Journal of Language Resources and Evaluation, 42(4), 335-359.  
shortdesc: An acted, multimodal and multispeaker database containing approximately 12 hours of audiovisual data, including video, speech, motion capture of face, text transcriptions.  
summary: The IEMOCAP dataset consists of dyadic sessions where actors perform improvisations or scripted scenarios, specifically selected to elicit emotional expressions. IEMOCAP database is annotated by multiple annotators into categorical labels, such as anger, happiness, sadness, neutrality, as well as dimensional labels such as valence, activation and dominance. The detailed motion capture information, the interactive setting to elicit authentic emotions, and the size of the database make this corpus a valuable addition to the existing databases in the community for the study and modeling of multimodal and expressive human communication.  
<!--
tests:
coverage:
reviews:
-->
